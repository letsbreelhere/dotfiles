#!/usr/bin/env python3

import requests
import sys
import os
import optparse
import openai

def chat(
        prompt=None,
        model=None,
        system_prompt=None,
        temperature=None,
        max_tokens=None
):
    model = "gpt-3.5-turbo" if model is None else model
    system_prompt = "" if system_prompt is None else system_prompt

    data = {
        "model": model,
        "stream": True,
        "messages": [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    }

    if temperature is not None:
        data['temperature'] = temperature

    if max_tokens is not None:
        data['max_tokens'] = max_tokens

    response = openai.ChatCompletion.create(**data)

    for chunk in response:
        if chunk['choices'][0]['finish_reason'] == 'stop':
            print("\n")
            break
        else:
            print(chunk['choices'][0]['delta']['content'], end='', flush=True)


def get_models():
    response = openai.Model.list()
    return [model['id'] for model in response['data']]

if __name__ == "__main__":
    parser = optparse.OptionParser()
    parser.add_option(
        '-t', '--temperature',
        action="store",
        dest="temperature",
        help="temperature",
        type="float"
    )
    parser.add_option(
        '-m', '--max_tokens',
        action="store",
        dest="max_tokens",
        help="max_tokens",
        type="int"
    )
    parser.add_option(
        '-s', '--system_prompt',
        action="store",
        dest="system_prompt",
        help="system_prompt"
    )
    parser.add_option(
        '-o', '--model',
        action="store",
        dest="model",
        help="model"
    )
    parser.add_option(
        '-l', '--list-models',
        action="store_true",
        dest="list_models",
        help="list models"
    )
    options, args = parser.parse_args()

    if options.list_models:
        for model in get_models():
            print(model)
        sys.exit(0)

    if len(args) < 1:
        prompt = sys.stdin.read()
    else:
        prompt = args[0]

    chat(
        prompt=prompt,
        model=options.model,
        temperature=options.temperature,
        max_tokens=options.max_tokens,
        system_prompt=options.system_prompt
    )
